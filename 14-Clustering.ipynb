{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nbakas/MachineLearning/blob/main/14-Clustering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we use KMeans and Hierarchical Clustering to group wines based on their chemical properties. This helps identify natural wine segments without needing quality labels. These segments can support targeted marketing by identifying wines that appeal to similar consumer preferences. They help refine pricing strategies by grouping wines of similar composition and perceived quality. In production, clustering reveals how chemical features relate to wine styles, guiding process adjustments for consistency and quality control.\n",
    "\n",
    "PCA and t-SNE are used to reduce dimensionality and visualize the clusters in 2D. They help us better understand the structure of the data and validate the separation between wine groups by projecting high-dimensional features into a visual format. PCA highlights directions of maximum variance, while t-SNE captures non-linear relationships, making cluster patterns more visible and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "  \n",
    "# fetch dataset \n",
    "# https://archive.ics.uci.edu/dataset/186/wine+quality\n",
    "online_retail = fetch_ucirepo(id=186) \n",
    "  \n",
    "# dataset (as pandas dataframes) \n",
    "X = online_retail.data.features \n",
    "y = online_retail.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(y.columns)\n",
    "print(np.sort(np.unique(y.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Importing the matplotlib library for plotting\n",
    "y_class_counts = y.value_counts()  # Counting the frequency of each class in y\n",
    "plt.figure(figsize=(10, 2))  # Setting the figure size for the plot\n",
    "ax = y_class_counts.plot(kind='bar')  # Creating a bar plot for class frequencies\n",
    "plt.title('Frequency per y class')  # Setting the title of the plot\n",
    "plt.xlabel('Class')  # Labeling the x-axis as 'Class'\n",
    "plt.ylabel('Frequency')  # Labeling the y-axis as 'Frequency'\n",
    "plt.xticks(rotation=0)  # Rotating x-ticks to 0 degrees for better readability\n",
    "for p in ax.patches:  # Iterating over each bar in the plot\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),  # Annotating each bar with its height\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points')  # Positioning the annotation\n",
    "plt.show()  # Displaying the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove classes 3,9 because the number of samples is too small\n",
    "# Remove classes 3 and 9\n",
    "y = y.squeeze() # Convert y from DataFrame to Series\n",
    "mask = ~y.isin([3, 9]) # Create a mask to keep only the rows where y is not 3 or 9\n",
    "\n",
    "# Apply the mask using loc to keep index alignment\n",
    "X = X.loc[mask] # Keep only the rows where y is not 3 or 9\n",
    "y = y.loc[mask] # Keep only the rows where y is not 3 or 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some columns to reduce the dimensionality of the data\n",
    "X = X.drop(columns=['fixed_acidity', 'volatile_acidity', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'sulphates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the remaining feature names\n",
    "features_names = X.columns\n",
    "features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe to understand the structure of the data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataframe to a numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_type = 'StandardScaler' # Select among 'StandardScaler', 'MinMaxScaler', 'MaxAbsScaler', 'NOscaling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaler_type != 'NOscaling':\n",
    "    if scaler_type == 'StandardScaler':\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        # The StandardScaler will scale the data to have mean 0 and standard deviation 1\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == 'MinMaxScaler':\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        # The MinMaxScaler will scale the data to a given range (default is 0 to 1)\n",
    "        scaler = MinMaxScaler()\n",
    "    elif scaler_type == 'MaxAbsScaler':\n",
    "        from sklearn.preprocessing import MaxAbsScaler\n",
    "        # The MaxAbsScaler will scale the data to the range [-1, 1] based on the maximum absolute value\n",
    "        scaler = MaxAbsScaler()\n",
    "    X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_scatter(X, y, features_names):\n",
    "    # Create a grid of subplots with dimensions based on the number of features\n",
    "    fig, axes = plt.subplots(len(features_names), len(features_names), figsize=(15, 15))\n",
    "    # Iterate over each feature for the x-axis\n",
    "    for i in range(len(features_names)):\n",
    "        # Iterate over each feature for the y-axis\n",
    "        for j in range(len(features_names)):\n",
    "            # Check if the current subplot is not on the diagonal\n",
    "            if i != j:\n",
    "                # Scatter plot of the data points for the i-th and j-th features, colored by cluster labels\n",
    "                scatter = axes[i, j].scatter(X[:, i], X[:, j], c=y, cmap='inferno', s=10)\n",
    "                legend1 = axes[i, j].legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "                axes[i, j].add_artist(legend1)\n",
    "                # Set the x-axis label to the i-th feature name\n",
    "                axes[i, j].set_xlabel(features_names[i])\n",
    "                # Set the y-axis label to the j-th feature name\n",
    "                axes[i, j].set_ylabel(features_names[j])\n",
    "                # Set the title of the subplot to show the i-th feature vs the j-th feature\n",
    "                axes[i, j].set_title(f'{features_names[i]} vs {features_names[j]}')\n",
    "            else:\n",
    "                # Plot the histogram of the i-th feature\n",
    "                axes[i, j].hist(X[:, i], bins=20)\n",
    "                # Set the title of the subplot to show the i-th feature\n",
    "                axes[i, j].set_title(features_names[i])\n",
    "                # Set the x-axis label to the i-th feature name\n",
    "                axes[i, j].set_xlabel(features_names[i])\n",
    "                # Set the y-axis label to 'Frequency'\n",
    "    # Adjust the layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scatter(X, y, features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe any strong physical cluster in the pairwise plots. Hence we proceed with Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing KMeans from sklearn.cluster for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# Import the silhouette_score function from sklearn.metrics\n",
    "from sklearn.metrics import silhouette_score  \n",
    "\n",
    "# Determine the optimal number of clusters using the silhouette score\n",
    "range_n_clusters = list(range(2, 11))  # Define a range of cluster numbers to evaluate, from 2 to 10\n",
    "best_n_clusters = 2  # Initialize the best number of clusters with the minimum value in the range\n",
    "best_silhouette_score = -1  # Initialize the best silhouette score with a very low value\n",
    "\n",
    "for n_clusters in range_n_clusters:  # Iterate over the range of cluster numbers\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)  # Initialize KMeans with the current number of clusters\n",
    "    kmeans.fit(X)  # Fit the KMeans model to the scaled data\n",
    "    cluster_labels = kmeans.labels_  # Retrieve the cluster labels assigned to each data point\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)  # Calculate the average silhouette score for the current clustering\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")  # Print the silhouette score for the current number of clusters\n",
    "    if silhouette_avg > best_silhouette_score:  # Check if the current silhouette score is better than the best one found so far\n",
    "        best_silhouette_score = silhouette_avg  # Update the best silhouette score\n",
    "        best_n_clusters = n_clusters  # Update the best number of clusters\n",
    "        best_cluster_labels = kmeans.labels_  # Retrieve the cluster labels assigned to each data point\n",
    "        best_cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=features_names) # This gives you a table: feature values per cluster center. \n",
    "        best_cluster_features_stds = pd.DataFrame([X[best_cluster_labels == i].std(axis=0) for i in range(best_n_clusters)], columns=features_names) # This gives you a table: std of each feature per cluster.\n",
    "print(f\"The optimal number of clusters is {best_n_clusters} with a silhouette score of {best_silhouette_score:.2f}\")  # Print the optimal number of clusters and the corresponding silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_cluster_features_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables best_cluster_centers and best_cluster_features_stds are useful for understanding the characteristics of each cluster.\n",
    "- best_cluster_centers: This DataFrame contains the feature values for the center of each cluster. It helps in identifying the typical or average feature values that define each cluster, providing insights into the central tendency of the data points within each cluster.\n",
    "- best_cluster_features_stds: This DataFrame contains the standard deviation of each feature within each cluster. It is useful for understanding the variability or spread of the data points around the cluster centers. A lower standard deviation indicates that the data points are closely packed around the cluster center, while a higher standard deviation suggests more dispersion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scatter(X, best_cluster_labels, features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions for hierarchical clustering and silhouette score calculation\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "# The linkage function from scipy.cluster.hierarchy is typically used for agglomerative clustering. The method='ward' specifies that the Ward's method is used to minimize the variance within each cluster.\n",
    "linkage_matrix = linkage(X, method='ward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the Linkage Matrix:\n",
    "\n",
    "The result is a NumPy array of shape (n-1, 4), where n is the number of original observations (data points). Each row corresponds to one merge step in the clustering process. Each row has 4 columns:\n",
    "\n",
    "| Column Index | Description                                            |\n",
    "| ------------ | ------------------------------------------------------ |\n",
    "| `Z[i, 0]`    | Index of the first cluster merged                      |\n",
    "| `Z[i, 1]`    | Index of the second cluster merged                     |\n",
    "| `Z[i, 2]`    | Distance (dissimilarity) between the merged clusters   |\n",
    "| `Z[i, 3]`    | Number of original samples in the newly formed cluster |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns: [Cluster1, Cluster2, SampleCount, Distance]\n",
    "from copy import deepcopy\n",
    "cols = deepcopy(linkage_matrix[:, [0, 1, 3, 2]])\n",
    "\n",
    "# Select first and last 5 rows\n",
    "subset = np.vstack([cols[:5], cols[-5:]])\n",
    "\n",
    "# Print header\n",
    "print(f\"{'Cluster 1':>10} {'Cluster 2':>10} {'Count':>10} {'Distance':>10}\")\n",
    "\n",
    "# Format and print each row\n",
    "for row in subset:\n",
    "    c1, c2, count, dist = int(row[0]), int(row[1]), int(row[2]), float(row[3])\n",
    "    print(f\"{c1:10} {c2:10} {count:10} {dist:10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the silhouette score\n",
    "range_n_clusters = list(range(2, 11))  # Define a range of cluster numbers to evaluate, from 2 to 10\n",
    "best_n_clusters_hierarchical = 2  # Initialize the best number of clusters with the minimum value in the range\n",
    "best_silhouette_score_hierarchical = -1  # Initialize the best silhouette score with a very low value\n",
    "\n",
    "for n_clusters in range_n_clusters:  # Iterate over the range of cluster numbers\n",
    "    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')  # Retrieve the cluster labels assigned to each data point\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)  # Calculate the average silhouette score for the current clustering\n",
    "    print(f\"For n_clusters = {n_clusters}, the average silhouette_score is : {silhouette_avg}\")  # Print the silhouette score for the current number of clusters\n",
    "    if silhouette_avg > best_silhouette_score_hierarchical:  # Check if the current silhouette score is better than the best one found so far\n",
    "        best_cluster_labels = cluster_labels  # Update the best cluster labels\n",
    "        best_silhouette_score_hierarchical = silhouette_avg  # Update the best silhouette score\n",
    "        best_n_clusters_hierarchical = n_clusters  # Update the best number of clusters\n",
    "\n",
    "print(f\"The optimal number of clusters for hierarchical clustering is {best_n_clusters_hierarchical} with a silhouette score of {best_silhouette_score_hierarchical:.2f}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features_scatter(X, best_cluster_labels, features_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a part of the dendrogram\n",
    "# Create a figure with a specific size\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "# Generate a dendrogram from the linkage matrix, truncating the dendrogram to show only the last 30 merged clusters.\n",
    "# p is the number of last merged clusters to display. It controls the number of displayed leaf nodes.  Each leaf may represent a single data point or a cluster of data points, depending on the total number of original data points and the value of p. p is used with the truncate_mode='lastp' parameter.\n",
    "# Colors indicate different high-level clusters formed near the root of the hierarchy\n",
    "nof_merged_clusters = 50    \n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=nof_merged_clusters, leaf_font_size=10, show_leaf_counts=True)\n",
    "\n",
    "# Set the title of the dendrogram plot\n",
    "plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "\n",
    "# Label the x-axis as 'Sample index'\n",
    "plt.xlabel(f'Clusters formed in the last {nof_merged_clusters} merges (values = number of samples in each)')\n",
    "\n",
    "# Label the y-axis as 'Distance'\n",
    "plt.ylabel('Distance') # Distance is the distance between the two clusters that are merged at each step of the hierarchical clustering.\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  # Import PCA for dimensionality reduction\n",
    "\n",
    "# Fit KMeans with the optimal number of clusters\n",
    "best_n_clusters = 4\n",
    "kmeans = KMeans(n_clusters=best_n_clusters, random_state=42)  # Initialize KMeans with the optimal number of clusters and a random state for reproducibility\n",
    "kmeans.fit(X)  # Fit KMeans to the scaled data\n",
    "cluster_labels = kmeans.labels_  # Retrieve the cluster labels assigned to each data point\n",
    "\n",
    "# Perform PCA to reduce the data to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2, random_state=42)  # Initialize PCA to reduce the data to 2 dimensions\n",
    "X_pca = pca.fit_transform(X)  # Fit PCA to the scaled data and transform it\n",
    "X_pca.shape  # Output the shape of the PCA-transformed data\n",
    "# Check variance retained\n",
    "variance_ratios = pca.explained_variance_ratio_\n",
    "total_variance_retained = variance_ratios.sum()\n",
    "\n",
    "print(f\"Variance explained by each component: {variance_ratios}\")\n",
    "print(f\"Total variance retained with 2 components: {total_variance_retained:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with a specific size for the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a scatter plot of the PCA-reduced data\n",
    "colors = ['red', 'green', 'blue', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan'] \n",
    "\n",
    "# Create a scatter plot with custom colors\n",
    "for cluster in range(best_n_clusters):\n",
    "    # Scatter plot for each cluster\n",
    "    # X_pca[cluster_labels == cluster, 0] represents the PCA-reduced data points for the current cluster on the x-axis\n",
    "    # X_pca[cluster_labels == cluster, 1] represents the PCA-reduced data points for the current cluster on the y-axis\n",
    "    # color=colors[cluster] assigns a unique color to each cluster\n",
    "    # label=f'Cluster {cluster}' assigns a label to each cluster for the legend\n",
    "    # marker='o' specifies the marker style for the scatter plot\n",
    "    # s=20 sets the size of the markers in the scatter plot\n",
    "    plt.scatter(X_pca[cluster_labels == cluster, 0], X_pca[cluster_labels == cluster, 1], \n",
    "                color=colors[cluster], label=f'Cluster {cluster}', marker='o', s=20)\n",
    "\n",
    "# Set the title of the scatter plot\n",
    "plt.title('KMeans Clusters Visualized using PCA')\n",
    "\n",
    "# Label the x-axis as 'Principal Component 1'\n",
    "plt.xlabel('Principal Component 1')\n",
    "\n",
    "# Label the y-axis as 'Principal Component 2'\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE  # Import TSNE for t-SNE dimensionality reduction\n",
    "\n",
    "# Perform t-SNE to reduce the data to 2 dimensions for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)  # Initialize t-SNE to reduce the data to 2 dimensions\n",
    "X_tsne = tsne.fit_transform(X)  # Fit t-SNE to the scaled data and transform it\n",
    "\n",
    "# Create a figure with a specific size for the scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a scatter plot of the t-SNE-reduced data\n",
    "for cluster in range(best_n_clusters):\n",
    "    # Scatter plot for each cluster\n",
    "    plt.scatter(X_tsne[cluster_labels == cluster, 0], X_tsne[cluster_labels == cluster, 1], \n",
    "                color=colors[cluster], label=f'Cluster {cluster}', marker='o', s=20)\n",
    "\n",
    "# Set the title of the scatter plot\n",
    "plt.title('KMeans Clusters Visualized using t-SNE')\n",
    "\n",
    "# Label the x-axis as 't-SNE Component 1'\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "\n",
    "# Label the y-axis as 't-SNE Component 2'\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "# Add a legend to the plot\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=y=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://github.com/nbakas/MachineLearning/blob/main/08-BankingDataset.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "if not local_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    local_path.write_bytes(response.content)\n",
    "%run 08-BankingDataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training, validation, and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the numpy library for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import the train_test_split function from sklearn to split the data into training, validation, and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import the DecisionTreeClassifier class from sklearn to create and train a decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import the accuracy_score function from sklearn to evaluate the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we split the data into a temporary set (X_temp, y_temp) and a test set (X_test, y_test).\n",
    "# The test set will be 20% of the original data, ensuring that the model's performance is evaluated on unseen data.\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we split the temporary set (X_temp, y_temp) into training (X_train, y_train) and validation sets (X_val, y_val).\n",
    "# The validation set will be 25% of the temporary set, which corresponds to 20% of the original data.\n",
    "# This ensures that the training set is 60% of the original data, and the validation set is 20% of the original data.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# Create a decision tree classifier\n",
    "# max_depth is the maximum depth of the tree, it varies from 1 to infinity (default=None)\n",
    "# min_samples_leaf is the minimum number of samples required to be at a leaf node, it varies from 1 to infinity (default=2)\n",
    "# ccp_alpha is the complexity parameter for the cost-complexity pruning, it varies from 0.0 to infinity (default=0.0)\n",
    "# criterion is the function used to measure the quality of a split, it varies between 'gini', 'entropy', 'log_loss' (default='gini')\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_leaf=2, ccp_alpha=0.0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(f\"Training Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Make predictions on the validation set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# Make predictions on the test set, and evaluate the classifier\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the depth of the tree\n",
    "print(f\"Depth of the tree: {clf.get_depth()}\")\n",
    "\n",
    "# Print the minimum number of samples required to be at a leaf node\n",
    "print(f\"Min samples leaf: {clf.min_samples_leaf}\")\n",
    "\n",
    "# Print the complexity parameter for the cost-complexity pruning\n",
    "print(f\"Ccp alpha: {clf.ccp_alpha}\")\n",
    "\n",
    "# Print the function used to measure the quality of a split\n",
    "print(f\"Criterion: {clf.criterion}\")\n",
    "\n",
    "# Print the number of leaves of the tree\n",
    "print(f\"Number of leaves of the tree: {clf.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for visualization\n",
    "from sklearn.tree import plot_tree  # plot_tree is used to visualize the decision tree\n",
    "import matplotlib.pyplot as plt  # matplotlib.pyplot is used for creating static, animated, and interactive visualizations in Python\n",
    "\n",
    "# Check if the number of leaves in the decision tree is less than 10, to avoid plotting a large tree\n",
    "if clf.get_n_leaves() < 10:\n",
    "    # Set the size of the figure for better visualization\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    # Plot the decision tree with filled nodes, feature names, class names, and specified font size\n",
    "    plot_tree(clf,  # Plot the decision tree\n",
    "              filled=True,  # Fill the nodes with colors\n",
    "              feature_names=X.columns,  # Use the column names of the encoded features\n",
    "              class_names=np.unique(y.values),  # Use the unique values of the target variable as class names\n",
    "              fontsize=10)  # Set the font size for the text in the plot\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "else:\n",
    "    # Print a message indicating that the tree is too large to visualize\n",
    "    print(\"The tree is too large to visualize, number of leaves: \", clf.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the export_text function from sklearn.tree to export the decision tree rules as text\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Export the decision tree rules as text, using the feature names from the encoded columns\n",
    "tree_rules = export_text(clf, feature_names=list(X.columns))\n",
    "\n",
    "# Print the exported decision tree rules\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOF_FOLDS = 5 # Number of folds\n",
    "PERCENTAGE_TEST = 0.2 # Percentage of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of classes in the target variable\n",
    "print(f\"Number of instances in the target variable: {y.value_counts()}\")\n",
    "# Print the percentage of each class in the target variable\n",
    "# The normalize=True parameter in value_counts() calculates the relative frequencies of each class\n",
    "print(f\"Percentage of classes in the target variable: {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  # Split the data into training and testing sets\n",
    "                                                    y,  # The target variable\n",
    "                                                    test_size=PERCENTAGE_TEST,  # The proportion of the dataset to include in the test split\n",
    "                                                    random_state=42)  # Random state for reproducibility, so that if we run the code again, we will get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KFold from sklearn.model_selection for cross-validation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KFold with the number of splits, shuffle, and random state\n",
    "kf = KFold(n_splits=NOF_FOLDS,  # Initialize KFold with the number of splits\n",
    "           shuffle=True,  # Shuffle the data before splitting into batches\n",
    "           random_state=42)  # Random state for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to accumulate accuracy scores\n",
    "average_accuracy_train = 0\n",
    "average_accuracy_val = 0\n",
    "\n",
    "# Loop over each fold in the KFold split\n",
    "all_val_indices = []\n",
    "for fold_index, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Print the number and percentage of instances per class in the training fold\n",
    "    print(f\"Number of instances per class in the training fold: {y_train_fold.value_counts()}\")\n",
    "    print(f\"Percentage of instances per class in the training fold: {y_train_fold.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Print the number and percentage of instances per class in the validation fold\n",
    "    print(f\"Number of instances per class in the validation fold: {y_val_fold.value_counts()}\")\n",
    "    print(f\"Percentage of instances per class in the validation fold: {y_val_fold.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Initialize and train the DecisionTreeClassifier on the training fold\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=2, ccp_alpha=0.01)\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict and calculate accuracy for the training fold\n",
    "    y_pred_train = clf.predict(X_train_fold)\n",
    "    accuracy_train = accuracy_score(y_train_fold, y_pred_train)\n",
    "    print(f\"Training Accuracy: {accuracy_train:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "\n",
    "    # Predict and calculate accuracy for the validation fold\n",
    "    y_pred_val = clf.predict(X_val_fold)\n",
    "    accuracy_val = accuracy_score(y_val_fold, y_pred_val)\n",
    "    print(f\"Validation Accuracy: {accuracy_val:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "\n",
    "    # Accumulate the accuracy scores for averaging\n",
    "    average_accuracy_train += accuracy_train\n",
    "    average_accuracy_val += accuracy_val\n",
    "\n",
    "    all_val_indices.extend(val_index.tolist())  # Append the validation indices to the list\n",
    "\n",
    "    # Print a separator for readability\n",
    "    print(100*\"-\")\n",
    "\n",
    "# Print a separator for readability\n",
    "print(100*\"-\")\n",
    "# Print the first 10 validation indices after sorting them\n",
    "print(sorted(all_val_indices)[:10])\n",
    "print(100*\"-\")\n",
    "# Print the total number of validation indices and the number of instances in the training set, to check if they are the same\n",
    "print(f\"len(all_val_indices)={len(all_val_indices)}, X_train.shape[0]={X_train.shape[0]}\")\n",
    "print(100*\"-\")\n",
    "\n",
    "# Calculate and print the average training accuracy over all folds\n",
    "average_accuracy_train /= NOF_FOLDS\n",
    "print(f\"Average Training Accuracy: {average_accuracy_train:.5f}\")\n",
    "\n",
    "# Calculate and print the average validation accuracy over all folds\n",
    "average_accuracy_val /= NOF_FOLDS\n",
    "print(f\"Average Cross-validation Accuracy: {average_accuracy_val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratified Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds for cross-validation\n",
    "NOF_FOLDS = 5\n",
    "\n",
    "# Percentage of data to be used for testing\n",
    "PERCENTAGE_TEST = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split the data into train and test sets, using stratified sampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,  # Features to be split\n",
    "                                                    y,  # Target variable\n",
    "                                                    test_size=PERCENTAGE_TEST,  # Proportion of data for testing\n",
    "                                                    random_state=42,  # Seed for reproducibility\n",
    "                                                    stratify=y)  # Ensures class distribution is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of classes in the target variable\n",
    "print(f\"Number of instances in the training set: {y_train.value_counts()}\")\n",
    "print(f\"Percentage of classes in the training set: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"Number of instances in the test set: {y_test.value_counts()}\")\n",
    "print(f\"Percentage of classes in the test set: {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StratifiedKFold for stratified cross-validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Initialize StratifiedKFold with number of splits, shuffle, and random state\n",
    "kf = StratifiedKFold(n_splits=NOF_FOLDS, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize average training accuracy to zero\n",
    "average_accuracy_train = 0\n",
    "# Initialize average validation accuracy to zero\n",
    "average_accuracy_val = 0\n",
    "# Loop over each fold index and split indices for training and validation\n",
    "for fold_index, (train_index, val_index) in enumerate(kf.split(X_train, y_train)): # Note that now (stratedfied) we also use the target variable y_train to split the data\n",
    "    # Split the training data into training and validation folds\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    # Split the target data into training and validation folds\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Print the number of instances per class in the training fold\n",
    "    print(f\"Number of instances per class in the training fold: {y_train_fold.value_counts()}\")\n",
    "    # Print the number of instances per class in the validation fold\n",
    "    print(f\"Number of instances per class in the validation fold: {y_val_fold.value_counts()}\")\n",
    "\n",
    "    # Initialize DecisionTreeClassifier with specified parameters\n",
    "    clf = DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=2, ccp_alpha=0.01)\n",
    "    # Fit the classifier on the training fold\n",
    "    clf.fit(X_train_fold, y_train_fold)\n",
    "    # Predict on the training fold\n",
    "    y_pred_train = clf.predict(X_train_fold)\n",
    "    # Calculate training accuracy\n",
    "    accuracy_train = accuracy_score(y_train_fold, y_pred_train)\n",
    "    # Print training accuracy for the current fold\n",
    "    print(f\"Training Accuracy: {accuracy_train:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    # Predict on the validation fold\n",
    "    y_pred_val = clf.predict(X_val_fold)\n",
    "    # Calculate validation accuracy\n",
    "    accuracy_val = accuracy_score(y_val_fold, y_pred_val)\n",
    "    # Print validation accuracy for the current fold\n",
    "    print(f\"Validation Accuracy: {accuracy_val:.5f}, fold {fold_index+1} of {NOF_FOLDS}\")\n",
    "    \n",
    "    # Accumulate training accuracy\n",
    "    average_accuracy_train += accuracy_train\n",
    "    # Accumulate validation accuracy\n",
    "    average_accuracy_val += accuracy_val\n",
    "\n",
    "    # Print separator line\n",
    "    print(100*\"-\")\n",
    "\n",
    "# Print separator line\n",
    "print(100*\"-\")\n",
    "# Calculate average training accuracy over all folds\n",
    "average_accuracy_train /= NOF_FOLDS\n",
    "# Print average training accuracy\n",
    "print(f\"Average Training Accuracy: {average_accuracy_train:.5f}\")\n",
    "# Calculate average validation accuracy over all folds\n",
    "average_accuracy_val /= NOF_FOLDS\n",
    "# Print average cross-validation accuracy\n",
    "print(f\"Average Cross-validation Accuracy: {average_accuracy_val:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

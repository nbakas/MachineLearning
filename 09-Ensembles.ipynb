{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is downloading the notebook from GitHub and running it\n",
    "import requests\n",
    "from pathlib import Path\n",
    "url = \"https://raw.githubusercontent.com/nbakas/MachineLearning/refs/heads/main/08-BankingDataset.ipynb\"\n",
    "filename = url.split(\"/\")[-1]\n",
    "local_path = Path.cwd() / filename\n",
    "if not local_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    local_path.write_bytes(response.content)\n",
    "%run 08-BankingDataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of X,y, to speed up the computation. X,y are pandas DataFrames\n",
    "X_subset = X.sample(n=10_000, random_state=42)\n",
    "y_subset = y.loc[X_subset.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, stratify=y_subset, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 500],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [10, 20],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [10, 20]  # Minimum number of samples required to be at a leaf node\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier and StratifiedKFold\n",
    "# StratifiedKFold is used to ensure that the training and test sets have the same proportion of classes as the original dataset\n",
    "rf = RandomForestClassifier(random_state=42) # Initialize the classifier\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Initialize the StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV with return_train_score=True\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf, # The classifier\n",
    "    param_grid=param_grid, # The parameter grid\n",
    "    cv=skf, # The StratifiedKFold\n",
    "    scoring='roc_auc', # The scoring metric\n",
    "    n_jobs=-1, # The number of jobs to run in parallel\n",
    "    verbose=4, # The verbosity level\n",
    "    return_train_score=True # Whether to return the training scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = best_rf.feature_importances_ # Get the feature importances\n",
    "sorted_indices = np.argsort(importances)[::-1] # Sort the feature importances in descending order\n",
    "sorted_importances = importances[sorted_indices] # Get the sorted feature importances\n",
    "sorted_features = X_train.columns[sorted_indices] # Get the sorted features\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6)) # Create a figure\n",
    "plt.barh(sorted_features, sorted_importances) # Plot the feature importance\n",
    "plt.xlabel('Importance') # Label the x-axis\n",
    "plt.ylabel('Feature') # Label the y-axis\n",
    "plt.title('Feature Importance') # Title the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = best_rf.predict(X_test) # Predict the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred) # Calculate the accuracy of the test set\n",
    "classification_report_str = classification_report(y_test, y_pred) # Calculate the classification report\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_) # Print the best hyperparameters\n",
    "print(\"Test set accuracy: {:.4f}\".format(test_accuracy)) # Print the accuracy of the test set\n",
    "print(\"\\nClassification Report:\\n\", classification_report_str) # Print the classification report\n",
    "\n",
    "\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred, normalize='true') * 100 # Calculate the confusion matrix\n",
    "confusion_matrix_raw = confusion_matrix(y_test, y_pred) # Calculate the confusion matrix\n",
    "\n",
    "# Combine percentage and raw values\n",
    "annot = np.empty_like(confusion_matrix_result, dtype=object) # Create an empty array to store the annotations\n",
    "for i in range(confusion_matrix_result.shape[0]): # Loop through the rows of the confusion matrix\n",
    "    for j in range(confusion_matrix_result.shape[1]): # Loop through the columns of the confusion matrix\n",
    "        annot[i, j] = f\"{confusion_matrix_result[i, j]:.2f}%({confusion_matrix_raw[i, j]})\" # Store the annotations\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8)) # Create a figure\n",
    "import seaborn as sns # Import seaborn\n",
    "sns.heatmap(confusion_matrix_result, annot=annot, fmt='', cmap='Blues', xticklabels=y_test.iloc[:, 0].unique(), yticklabels=y_test.iloc[:, 0].unique()) # Plot the confusion matrix\n",
    "plt.xlabel('Predicted') # Label the x-axis\n",
    "plt.ylabel('Actual') # Label the y-axis\n",
    "plt.title('Confusion Matrix') # Title the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and cross-validation accuracy\n",
    "results = grid_search.cv_results_ # Get the results from the grid search\n",
    "mean_train_scores = results['mean_train_score'] # Get the mean training scores\n",
    "mean_test_scores = results['mean_test_score'] # Get the mean cross-validation scores\n",
    "iso = np.argsort(mean_test_scores) # Sort the cross-validation scores\n",
    "plt.figure(figsize=(14, 6)) # Create a figure\n",
    "plt.plot(mean_train_scores[iso], label='Train Accuracy', marker='o') # Plot the training accuracy\n",
    "plt.plot(mean_test_scores[iso], label='CV Accuracy', marker='s') # Plot the cross-validation accuracy\n",
    "plt.xlabel('Hyperparameter Combination Index') # Label the x-axis\n",
    "plt.ylabel('Accuracy') # Label the y-axis\n",
    "plt.ylim(0.85, 0.95)  # Set y limits from 0 to 1\n",
    "plt.title('Train vs Cross-Validation Accuracy') # Title the plot\n",
    "plt.legend() # Show the legend\n",
    "plt.grid(True) # Show the grid\n",
    "plt.tight_layout() # Adjust the layout\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the precision of the model on the test set\n",
    "test_precision = precision_score(y_test, y_test_pred, pos_label='yes')\n",
    "\n",
    "# Calculate the recall of the model on the test set\n",
    "test_recall = recall_score(y_test, y_test_pred, pos_label='yes')\n",
    "\n",
    "# Calculate the F1-score of the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred, pos_label='yes')\n",
    "\n",
    "# Calculate the ROC AUC score of the model on the test set\n",
    "y_test_prob = best_rf.predict_proba(X_test)[:, 1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# Print all metrics for the test set\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Set Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Set Recall: {test_recall:.4f}\")\n",
    "print(f\"Test Set F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Set ROC AUC: {test_roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple binary mapping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200], # Number of trees in the forest\n",
    "    'max_depth': [3, 6, 10], # Maximum depth of the tree\n",
    "    'learning_rate': [0.01, 0.1], # Learning rate\n",
    "    'subsample': [0.8, 1.0] # Subsample\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and cross-validation\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42) # Initialize the XGBoost classifier\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # Initialize the StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb, # The classifier\n",
    "    param_grid=param_grid, # The parameter grid\n",
    "    cv=skf, # The StratifiedKFold\n",
    "    scoring='accuracy', # The scoring metric\n",
    "    n_jobs=-1, # The number of jobs to run in parallel\n",
    "    verbose=1, # The verbosity level\n",
    "    return_train_score=True # Whether to return the training scores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "best_xgb = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importances = best_xgb.feature_importances_ # Get the feature importances\n",
    "sorted_indices = np.argsort(importances)[::-1] # Sort the feature importances in descending order\n",
    "sorted_importances = importances[sorted_indices] # Get the sorted feature importances\n",
    "sorted_features = X_train.columns[sorted_indices] # Get the sorted features\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6)) # Create a figure\n",
    "plt.barh(sorted_features, sorted_importances) # Plot the feature importance\n",
    "plt.xlabel('Importance') # Label the x-axis\n",
    "plt.ylabel('Feature') # Label the y-axis\n",
    "plt.title('Feature Importance') # Title the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_pred = best_xgb.predict(X_test) # Predict the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred) # Calculate the accuracy of the test set\n",
    "classification_report_str = classification_report(y_test, y_pred) # Calculate the classification report\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_) # Print the best hyperparameters\n",
    "print(\"Test set accuracy: {:.4f}\".format(test_accuracy)) # Print the accuracy of the test set\n",
    "print(\"\\nClassification Report:\\n\", classification_report_str) # Print the classification report\n",
    "\n",
    "\n",
    "confusion_matrix_result = confusion_matrix(y_test, y_pred, normalize='true') * 100 # Calculate the confusion matrix\n",
    "confusion_matrix_raw = confusion_matrix(y_test, y_pred) # Calculate the confusion matrix\n",
    "\n",
    "# Combine percentage and raw values\n",
    "annot = np.empty_like(confusion_matrix_result, dtype=object) # Create an empty array to store the annotations\n",
    "for i in range(confusion_matrix_result.shape[0]): # Loop through the rows of the confusion matrix\n",
    "    for j in range(confusion_matrix_result.shape[1]): # Loop through the columns of the confusion matrix\n",
    "        annot[i, j] = f\"{confusion_matrix_result[i, j]:.2f}%({confusion_matrix_raw[i, j]})\" # Store the annotations\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8)) # Create a figure\n",
    "import seaborn as sns # Import seaborn\n",
    "sns.heatmap(confusion_matrix_result, annot=annot, fmt='', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test)) # Plot the confusion matrix\n",
    "plt.xlabel('Predicted') # Label the x-axis\n",
    "plt.ylabel('Actual') # Label the y-axis\n",
    "plt.title('Confusion Matrix') # Title the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and cross-validation accuracy\n",
    "results = grid_search.cv_results_ # Get the results from the grid search\n",
    "mean_train_scores = results['mean_train_score'] # Get the mean training scores\n",
    "mean_test_scores = results['mean_test_score'] # Get the mean cross-validation scores\n",
    "iso = np.argsort(mean_test_scores) # Sort the cross-validation scores\n",
    "plt.figure(figsize=(14, 6)) # Create a figure\n",
    "plt.plot(mean_train_scores[iso], label='Train Accuracy', marker='o') # Plot the training accuracy\n",
    "plt.plot(mean_test_scores[iso], label='CV Accuracy', marker='s') # Plot the cross-validation accuracy\n",
    "plt.xlabel('Hyperparameter Combination Index') # Label the x-axis\n",
    "plt.ylabel('Accuracy') # Label the y-axis\n",
    "plt.ylim(0.85, 1.05)  # Set y limits from 0 to 1\n",
    "plt.title('Train vs Cross-Validation Accuracy') # Title the plot\n",
    "plt.legend() # Show the legend\n",
    "plt.grid(True) # Show the grid\n",
    "plt.tight_layout() # Adjust the layout\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "# Predict the labels for the test set\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the precision of the model on the test set\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the recall of the model on the test set\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the F1-score of the model on the test set\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "\n",
    "# Calculate the ROC AUC score of the model on the test set\n",
    "y_test_prob = best_xgb.predict_proba(X_test)[:, 1]\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# Print all metrics for the test set\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Set Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Set Recall: {test_recall:.4f}\")\n",
    "print(f\"Test Set F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test Set ROC AUC: {test_roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
